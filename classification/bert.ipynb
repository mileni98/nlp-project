{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, TrainingArguments, Trainer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mlflow_experiment(tracking_url, experiment_name):\n",
    "\n",
    "    # Set the tracking uri  and the active experiment \n",
    "    mlflow.set_tracking_uri(tracking_url)\n",
    "\n",
    "    # Set the current active experiment and return the experiment metadata\n",
    "    return mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_pred, y_test):\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_balanced = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate metrics for negative class\n",
    "    precision_neg = precision_score(y_test, y_pred, pos_label = 0)\n",
    "    recall_neg = recall_score(y_test, y_pred, pos_label = 0)\n",
    "    f1_neg = f1_score(y_test, y_pred, pos_label = 0)\n",
    "\n",
    "    # Calculate metrics for positive class\n",
    "    precision_pos = precision_score(y_test, y_pred, pos_label = 1)\n",
    "    recall_pos = recall_score(y_test, y_pred, pos_label = 1)\n",
    "    f1_pos = f1_score(y_test, y_pred, pos_label = 1)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Extract TP, FP, TN, FN from confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    return accuracy, accuracy_balanced, precision_neg, recall_neg, f1_neg, precision_pos, recall_pos, f1_pos, tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics_to_mlflow(metrics):\n",
    "\n",
    "    # Log the accuracy\n",
    "    mlflow.log_metric('accuracy', metrics[0])\n",
    "    mlflow.log_metric('accuracy_balanced', metrics[1])\n",
    "\n",
    "    # Log metrics for negative classes\n",
    "    mlflow.log_metric('precision_neg', metrics[2])\n",
    "    mlflow.log_metric('recall_neg', metrics[3])\n",
    "    mlflow.log_metric('f1_neg', metrics[4])\n",
    "\n",
    "    # Log metrics for positive classes\n",
    "    mlflow.log_metric('precision_pos', metrics[5])\n",
    "    mlflow.log_metric('recall_pos', metrics[6])\n",
    "    mlflow.log_metric('f1_pos', metrics[7])\n",
    "\n",
    "    # Log the confusion matrix elements\n",
    "    mlflow.log_metric('tp', metrics[8])\n",
    "    mlflow.log_metric('fp', metrics[9])\n",
    "    mlflow.log_metric('tn', metrics[10])\n",
    "    mlflow.log_metric('fn', metrics[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_to_mlflow(model, artifact_path, train_dataset, run_name):\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(train_dataset, model.predict(train_dataset))\n",
    "\n",
    "    # Log the model to mlflow\n",
    "    mlflow.pytorch.log_model(\n",
    "        pytorch_model = model,\n",
    "        artifact_path = artifact_path,\n",
    "        signature = signature,\n",
    "        input_example = train_dataset,\n",
    "        registered_model_name = run_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_split_dataset(csv_name, path):\n",
    "\n",
    "    # Read the excel file into the corresponding DataFrame\n",
    "    dataset = pd.read_csv(path + csv_name)\n",
    "\n",
    "    # Replace sentiment values with binary values\n",
    "    dataset['sentiment'] = dataset['sentiment'].replace({'negative':0, 'positive':1})\n",
    "\n",
    "    # Rename columns in the DataFrame\n",
    "    dataset = dataset.rename(columns={'review' : 'text', 'sentiment' : 'labels'})\n",
    "\n",
    "    # Split data into training and testing datasets\n",
    "    train_data, test_data = train_test_split(dataset, test_size = 0.2, random_state = 43)\n",
    "\n",
    "    # Further split training data into train and validation sets\n",
    "    train_data, val_data = train_test_split(train_data, test_size = 0.1, random_state = 43)\n",
    "\n",
    "    # Create a Dataset dictionary and convert dataframes to datasets\n",
    "    combined_dataset = DatasetDict({\n",
    "        'train': Dataset.from_pandas(train_data),\n",
    "        'val': Dataset.from_pandas(val_data),\n",
    "        'test': Dataset.from_pandas(test_data)\n",
    "    })\n",
    "\n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(combined_dataset, tokenizer):\n",
    "\n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        # max_length padding ensures same length of each sequence within a batch, truncation parameter doesn't play a role\n",
    "        return tokenizer(examples['text'], truncation = True, padding = 'max_length')\n",
    "\n",
    "    # Tokenize the combined dataset, batched parameter tokenizes multiple samples simultaneously\n",
    "    tokenized_dataset = combined_dataset.map(tokenize_function, batched = True)\n",
    "\n",
    "    # Return small subsets of dataset for testing\n",
    "    train_dataset = tokenized_dataset['train'].shuffle(seed = 42).select(range(10))\n",
    "    eval_dataset = tokenized_dataset['val'].shuffle(seed = 42).select(range(10))\n",
    "    test_dataset = tokenized_dataset['test'].shuffle(seed = 42).select(range(10))\n",
    "    \n",
    "    # Uncomment Return split datasets\n",
    "    #train_dataset = tokenized_dataset['train']\n",
    "    #eval_dataset = tokenized_dataset['val']\n",
    "    #test_dataset = tokenized_dataset['test']\n",
    "\n",
    "    return train_dataset, eval_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_trainer_and_model(train_dataset, eval_dataset, tokenizer, checkpoint, training_args):\n",
    "\n",
    "    # Create a data collator for padding\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    # Load the pretrained BERT model for sequence classification\n",
    "    model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels = 2)\n",
    "    \n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = eval_dataset,\n",
    "        tokenizer = tokenizer,\n",
    "        data_collator = data_collator,\n",
    "    )\n",
    "    \n",
    "    return trainer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_test_and_log(trainer, model, data_name, artifact_path, train_dataset, test_dataset):\n",
    "    \n",
    "    # Start an MLflow run using the previously defined name\n",
    "    with mlflow.start_run(run_name = 'bert'):\n",
    "      \n",
    "        # Log additional useful parameters\n",
    "        mlflow.log_param('data_name', data_name)\n",
    "\n",
    "        # Train the model using the pretrained trainer\n",
    "        trainer.train()\n",
    "\n",
    "        # Make predictions on the test dataset, y_pred contains list of probabilities for each sentiment class \n",
    "        # For example - [0.85, 0.15] Probability for positive sentiment is 85% and for negative is 15%\n",
    "        y_pred = trainer.predict(test_dataset)\n",
    "\n",
    "        # Extract predicted lables, get the class with highest probability\n",
    "        preds = np.argmax(y_pred.predictions, axis=-1)\n",
    "\n",
    "        # DELETE THIS\n",
    "        print('y_pred is: ', y_pred)\n",
    "        print('preds is: ', preds)\n",
    "        print('y_pred.label_ids is: ', y_pred.label_ids)\n",
    "        print('y_pred.predictions is: ', y_pred.predictions)\n",
    "        \n",
    "        # Calculate metrics by comparing real label and predicted label\n",
    "        metrics = calculate_metrics(y_pred.label_ids, preds)\n",
    "        \n",
    "        # Log metrics to mlflow\n",
    "        log_metrics_to_mlflow(metrics)\n",
    "        \n",
    "        # Log the model to mlflow\n",
    "        log_model_to_mlflow(model, artifact_path, train_dataset, 'bert')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the pretrained bert model that will be used\n",
    "checkpoint = 'bert-base-cased'\n",
    "\n",
    "# Set parameters needed to connect to mlflow\n",
    "# tracking_url, experiment_name = 'http://127.0.0.1:8088', 'aleksa_praksa'\n",
    "tracking_url, experiment_name = 'http://192.168.66.221:20002', 'aleksa_praksa'\n",
    "\n",
    "# Set folder path to the original dataset\n",
    "path, file_name, artifact_path = '../data/', 'imdb_dataset.csv', 'artifact'\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'output',\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 3,\n",
    "    logging_dir = './logs',\n",
    "    logging_steps = 100,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the mlflow experiment\n",
    "set_mlflow_experiment(tracking_url, experiment_name)\n",
    "\n",
    "# Preprocess dataset\n",
    "combined_dataset = prepare_and_split_dataset(file_name, path)\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "train_dataset, eval_dataset, test_dataset = tokenize_data(combined_dataset, tokenizer)\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer, model = initialize_trainer_and_model(train_dataset, eval_dataset, tokenizer, checkpoint, training_args)\n",
    "\n",
    "# Train the classifier and test it on the test dataset while logging relevant information\n",
    "ml_test_and_log(trainer, model, file_name, artifact_path, train_dataset, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
