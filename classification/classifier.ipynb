{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install additional libraires from README file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_mlflow_experiment(tracking_url, experiment_name):\n",
    "\n",
    "    # Set the tracking uri  and the active experiment \n",
    "    mlflow.set_tracking_uri(tracking_url)\n",
    "\n",
    "    # Set the current active experiment and return the experiment metadata\n",
    "    return mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(X_train, X_test, max_feature_no):\n",
    "    \n",
    "    # Vectorize train and test data using Tf-Idf\n",
    "    vectorizer = TfidfVectorizer(max_features = max_feature_no)\n",
    "    X_train_tf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    # Test data doesn't need fitting\n",
    "    X_test_tf = vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_tf, X_test_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_pred, y_test):\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_balanced = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate metrics for negative class\n",
    "    precision_neg = precision_score(y_test, y_pred, pos_label = 0)\n",
    "    recall_neg = recall_score(y_test, y_pred, pos_label = 0)\n",
    "    f1_neg = f1_score(y_test, y_pred, pos_label = 0)\n",
    "\n",
    "    # Calculate metrics for positive class\n",
    "    precision_pos = precision_score(y_test, y_pred, pos_label = 1)\n",
    "    recall_pos = recall_score(y_test, y_pred, pos_label = 1)\n",
    "    f1_pos = f1_score(y_test, y_pred, pos_label = 1)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Extract TP, FP, TN, FN from confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    return accuracy, accuracy_balanced, precision_neg, recall_neg, f1_neg, precision_pos, recall_pos, f1_pos, tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics_to_mlflow(metrics):\n",
    "\n",
    "    # Log the accuracy\n",
    "    mlflow.log_metric('accuracy', metrics[0])\n",
    "    mlflow.log_metric('accuracy_balanced', metrics[1])\n",
    "\n",
    "    # Log metrics for negative classes\n",
    "    mlflow.log_metric('precision_neg', metrics[2])\n",
    "    mlflow.log_metric('recall_neg', metrics[3])\n",
    "    mlflow.log_metric('f1_neg', metrics[4])\n",
    "\n",
    "    # Log metrics for positive classes\n",
    "    mlflow.log_metric('precision_pos', metrics[5])\n",
    "    mlflow.log_metric('recall_pos', metrics[6])\n",
    "    mlflow.log_metric('f1_pos', metrics[7])\n",
    "\n",
    "    # Log the confusion matrix elements\n",
    "    mlflow.log_metric('tp', metrics[8])\n",
    "    mlflow.log_metric('fp', metrics[9])\n",
    "    mlflow.log_metric('tn', metrics[10])\n",
    "    mlflow.log_metric('fn', metrics[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_to_mlflow(classifier, X_train, run_name):\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, classifier.predict(X_train))\n",
    "\n",
    "    # Log the model to mlflow\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model = classifier,\n",
    "        artifact_path = 'artifact',\n",
    "        signature = signature,\n",
    "        input_example = X_train,\n",
    "        registered_model_name = run_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(csv_name, path, tfidf_max_feature_no):\n",
    "\n",
    "    # Read the excel file into the corresponding DataFrame\n",
    "    dataset = pd.read_csv(path + csv_name)\n",
    "\n",
    "    # Split data into training and testing datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(dataset['cleaned_review'], dataset['sentiment'], test_size = 0.2, random_state = 43)\n",
    "\n",
    "    # Vectorize data using Tf-Idf\n",
    "    X_train_tr, X_test_tr = vectorize_data(X_train, X_test, tfidf_max_feature_no)   \n",
    "\n",
    "    return X_train_tr, y_train, X_test_tr, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tune_cv(classifier_name, param_space, X_train, y_train, jobs):\n",
    "\n",
    "    # Define cross-validation split while remaining same class balance\n",
    "    kf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 42)\n",
    "\n",
    "    # Initialize BayesSearchCV for hyperparameter tuning, change verbose to 1 to see output info\n",
    "    search = BayesSearchCV(estimator = classifier_name(), search_spaces = param_space, n_iter = 100, cv = kf, scoring = 'f1', verbose = 1, n_jobs = jobs, random_state = 42)\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Retrieve the best F1 score obtained during hyperparameter tuning\n",
    "    best_score = search.best_score_   \n",
    "\n",
    "    # Create a classifier instance using the best parameters\n",
    "    classifier = classifier_name(**search.best_params_)\n",
    "\n",
    "    return classifier, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_test_and_log(classifier, data_name, tfidf_feature_no, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # Make custom run names for each run to differentiate them on mlflow\n",
    "    run_name = f\"{classifier.__class__.__name__}_{os.path.splitext(data_name)[0]}_{tfidf_feature_no}\"\n",
    "\n",
    "   # Start an MLflow run using the previously defined name\n",
    "    with mlflow.start_run(run_name = run_name):\n",
    "\n",
    "        # Log the hyperparameters\n",
    "        mlflow.log_params(classifier.get_params())\n",
    "        \n",
    "        # Log additional useful parameters\n",
    "        mlflow.log_param('data_name', data_name)\n",
    "        mlflow.log_param('tfidf_features', tfidf_feature_no)\n",
    "\n",
    "        # Train the model\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = classifier.predict(X_test)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(y_pred, y_test)\n",
    "        \n",
    "        # Log metrics to mlflow\n",
    "        log_metrics_to_mlflow(metrics)\n",
    "\n",
    "        # Log the model to mlflow\n",
    "        log_model_to_mlflow(classifier, X_train, run_name)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters needed to connect to mlflow\n",
    "# tracking_url, experiment_name = 'http://127.0.0.1:8088', 'aleksa_praksa'\n",
    "tracking_url, experiment_name = 'http://192.168.66.221:20002', 'aleksa_praksa'\n",
    "\n",
    "# Set folder path to the preprocessed datasets\n",
    "path, datasets_list = '../data/', []\n",
    "\n",
    "# Define the file names of different preprocessed datasets\n",
    "for file in os.listdir(path):\n",
    "    if file.startswith('split') and file.endswith('.csv'):\n",
    "        datasets_list.append(file)\n",
    "\n",
    "# Define different values for the 'max_features' parameter in tf-idf\n",
    "tfidf_features_list = [10000, 20000]\n",
    "\n",
    "# Define classifiers and their hyperparameter spaces\n",
    "classifiers_list = [\n",
    "    {\n",
    "        # Baseline quick algorithm - Logistic Regression\n",
    "        'name': LogisticRegression,\n",
    "        'datasets_list_element': datasets_list,\n",
    "        'tfidf_features_list_element': tfidf_features_list,\n",
    "        'jobs': -1,\n",
    "        'param_space': {\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "            'C': (1e-4, 1e+4, 'log-uniform'), \n",
    "            'penalty': ['l2'],\n",
    "            'max_iter': (100, 1000),\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        # XGBoost Classifier\n",
    "        'name': XGBClassifier,\n",
    "        'datasets_list_element': ['split_F_spel_F_lem.csv', 'split_T_spel_F_stem.csv'],\n",
    "        'tfidf_features_list_element': [10000, 20000],\n",
    "        'jobs': 1,\n",
    "        'param_space': {\n",
    "            'n_estimators': [200],\n",
    "            'learning_rate': [0.01],\n",
    "            'max_depth': [15],\n",
    "            # 'n_jobs': [-1],\n",
    "        }\n",
    "    }, \n",
    "    {    \n",
    "        # Random Forest Classifier\n",
    "        'name': RandomForestClassifier,\n",
    "        'datasets_list_element': datasets_list,\n",
    "        'tfidf_features_list_element': tfidf_features_list,\n",
    "        'jobs': -1,\n",
    "        'param_space': {\n",
    "            'n_estimators': (10, 200),\n",
    "            'max_features': ['sqrt', 'log2'], \n",
    "            'max_depth': (5, 15),\n",
    "            'min_samples_split': (2, 5),\n",
    "            'min_samples_leaf': (1, 3),\n",
    "            'bootstrap': [True, False],\n",
    "        }\n",
    "    },  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms Bake Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/26 17:16:19 INFO mlflow.tracking.fluent: Experiment with name 'aleksa_praksa' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "# Set up the mlflow experiment\n",
    "set_mlflow_experiment(tracking_url, experiment_name)\n",
    "\n",
    "# Iterate over each classifier in the list\n",
    "for classifier_element in classifiers_list:\n",
    "    \n",
    "    # Extract classifier name and hyperparameter space\n",
    "    classifier_name = classifier_element['name']\n",
    "    datasets_list_element = classifier_element['datasets_list_element']\n",
    "    tfidf_features_list_element = classifier_element['tfidf_features_list_element']\n",
    "    jobs = classifier_element['jobs']\n",
    "    param_space = classifier_element['param_space']\n",
    "\n",
    "    # Iterate over each dataset in the list\n",
    "    for csv_name in datasets_list_element:\n",
    "\n",
    "        # Iterate over each value of max_feature tfidf\n",
    "        for no_feature in tfidf_features_list_element:\n",
    "\n",
    "            # Prepare the dataset using current CSV file and TF-IDF vectorize it using number of features\n",
    "            X_train_tr, y_train, X_test_tr, y_test = prepare_dataset(csv_name, path, no_feature)\n",
    "\n",
    "            # Check if it's XGBoost, if it is don't do hyperparameter tunning as there is only one combination of parameters\n",
    "            if classifier_name == XGBClassifier:\n",
    "                \n",
    "                # Instantiate the classifier directly\n",
    "                classifier = classifier_name()\n",
    "            else:\n",
    "                # Perform hyperparameter tuning using cross validation for the current classifier and get the best parameters\n",
    "                classifier, best_f1_score = hyperparameter_tune_cv(classifier_name, param_space, X_train_tr, y_train, jobs)\n",
    "            \n",
    "            # Train the classifier and test it on the test dataset while logging relevant information\n",
    "            ml_test_and_log(classifier, csv_name, no_feature, X_train_tr, y_train, X_test_tr, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
